{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1e77cb",
   "metadata": {},
   "source": [
    "# ðŸ§  News Classifier with Word2Vec + TF-IDF + StandardScaler + Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge data\n",
    "df = pd.read_csv(\"../../dataset/data.csv\")\n",
    "df['combined_text'] = df['title'] + ' ' + df['text'] + ' ' + df['subject']\n",
    "df.drop(['title', 'text', 'subject', 'date'], axis=1, inplace=True)\n",
    "\n",
    "X = df['combined_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_token_cleanup(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocessing_pipeline(text):\n",
    "    text = pre_token_cleanup(text)\n",
    "    tokens = word_tokenize(text, language=\"english\")\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "X_train = X_train_text.apply(preprocessing_pipeline)\n",
    "X_test = X_test_text.apply(preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec + TF-IDF\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def identity(x): return x\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=identity, min_df=0.01, max_df=0.9, ngram_range=(1, 3))\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "idf_weights = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding with weighted Word2Vec + StandardScaler\n",
    "def get_weighted_embedding(tokens, model, idf_dict):\n",
    "    vecs = [model.wv[word] * idf_dict[word] for word in tokens if word in model.wv and word in idf_dict]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "doc_embeddings_train = np.array([get_weighted_embedding(doc, w2v_model, idf_weights) for doc in X_train])\n",
    "doc_embeddings_test = np.array([get_weighted_embedding(doc, w2v_model, idf_weights) for doc in X_test])\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "doc_embeddings_train_std = scaler.fit_transform(doc_embeddings_train)\n",
    "doc_embeddings_test_std = scaler.transform(doc_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c253b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(doc_embeddings_train_std, y_train)\n",
    "y_pred = model.predict(doc_embeddings_test_std)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ceab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake News\", \"Real News\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "y_proba = model.predict_proba(doc_embeddings_test_std)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Fake News\", \"Real News\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3541b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "def plot_learning_curve_for_embeddings(estimator, title, X_raw, y, embedding_func, cv, n_jobs=1,\n",
    "                                       train_sizes=np.linspace(0.1, 0.9, 5)):\n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    for frac in train_sizes:\n",
    "        X_frac, _, y_frac, _ = train_test_split(X_raw, y, train_size=frac, random_state=42)\n",
    "        X_emb = np.array([embedding_func(doc, w2v_model, idf_weights) for doc in X_frac])\n",
    "        X_emb = scaler.fit_transform(X_emb)\n",
    "\n",
    "        inner_cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "        train_result, test_result = [], []\n",
    "\n",
    "        for train_idx, val_idx in inner_cv.split(X_emb):\n",
    "            X_train_cv, X_val_cv = X_emb[train_idx], X_emb[val_idx]\n",
    "            y_train_cv, y_val_cv = y_frac.iloc[train_idx], y_frac.iloc[val_idx]\n",
    "\n",
    "            estimator.fit(X_train_cv, y_train_cv)\n",
    "            train_result.append(estimator.score(X_train_cv, y_train_cv))\n",
    "            test_result.append(estimator.score(X_val_cv, y_val_cv))\n",
    "\n",
    "        train_scores.append(np.mean(train_result))\n",
    "        test_scores.append(np.mean(test_result))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_sizes, train_scores, 'o-', label='Training Score')\n",
    "    plt.plot(train_sizes, test_scores, 'o-', label='Cross-validation Score')\n",
    "    plt.xlabel(\"Training Set Fraction\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbb3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_for_embeddings(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    title=\"Learning Curve: Logistic Regression with W2V+TF-IDF+StandardScaler\",\n",
    "    X_raw=X_train,\n",
    "    y=y_train,\n",
    "    embedding_func=lambda doc: get_weighted_embedding(doc, w2v_model, idf_weights),\n",
    "    cv=ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
